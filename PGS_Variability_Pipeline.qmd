---
title: "PGS Variability Evaluation Pipeline - CAD"
author: "Sarah A Abramowitz"
date: last-modified
format:
  html:
    code-tools: true
    toc: true
    toc-depth: 6
    cap-location: top
    embed-resources: true
    citations-hover: true
fig-height: 4
execute:
  warning: false
  message: false
  echo: false
---

# Pipeline for Evaluating Population- and Individual- Level Performance of Polygenic Risk Scores

Scores generated with pgsc_calc, use znorm2

# Getting Started:

::: {.callout-note appearance="simple"}
There are four to-dos to customize and prepare your environment before you can proceed through the rest of the pipeline with relatively little modification:

1.  Install/load libraries
2.  Load custom theme
3.  Define global variables according to your environment and phenotype of study
4.  Create properly formatted data frame containing scores and other individual-level data
:::

## 1. Load libraries

```{r echo = T, results = 'hide'}
library(dplyr)
library(tidyverse)
library(vroom)
library(tidymodels)
library(ggplot2)
library(purrr)
library(tidyselect)
library(yardstick)
library(probably)
library(tidyposterior)
library(ggrepel)
library(workflowsets)
library(tune)
library(gt)
library(gtable)
library(gtsummary)
library(reshape2)
library(ggsci)
library(ggpubr)
tidymodels_prefer()
library(kableExtra)
devtools::install_github("mglev1n/calibratr") # if not yet installed
library(calibratr)

```

## 2. Load Custom Theme

```{r echo = T, results = 'hide'}
custom_theme <- theme_minimal() +
  theme(
    panel.background = element_rect(fill = "white"),
    text = element_text(family = "Arial", size = 12),
    plot.title = element_text(size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    axis.title.x = element_text(size = 14),
    axis.title.y = element_text(size = 14),
    axis.text.x = element_text(size = 12),
    axis.text.y = element_text(size = 12),
    plot.margin = margin(20, 20, 20, 20)
  )
```

## 3. Define global variables

Intended to improve generalizability of downstream code

-   pheno: define as your binary phenotype of interested (ie. "CAD"). By using the name PHENO_FLG for your outcome measure in the data frame, and defining your specific 'pheno' here, the pipeline will work for any binary phenotype outcome and replace PHENO_FLG with your phenotype of interest in figures.
-   database: define the database you are using (ie. "PMBB"). Outputted figure names and file paths will specify the biobank they are generated in
-   path: specify the desired destination for figures, tables, and r objects saved by this workflow

```{r}
pheno <- "CAD" # Replace "CAD" with your binary outcome of interest

database <- "PMBB" # Replace "PMBB" with the name of the biobank/population group you are working with

path <- "/project/damrauer_shared/Users/sabramow/PRS-Variability/results/" # Replace path with an existing path to a folder where you wish to store outputs

```

## 4. Create your data frame

::: callout-note
Requirements: data frame with at least the following columns

-   IID; <chr> specifying individual ID
-   PHENO_FLG; <fct> specifying binary no(0) or yes(1) disease
-   Sex; <chr> Male or Female
-   Age; <dbl>
-   PGS\_\_\_\_ ; for n PGSs being compared, should have n columns containing the Z_norm2 value generated by pgsc_calc, the titles of which should all have the prefix 'PGS'.
-   Ancestry; <chr> genetically determined ancestry

in future, will be options for additional covariates of interest
:::

Here is an example of of compiling data into a properly formatted data frame.

``` {{r}}
#PGSC_calc, when run with the ancestry option, will output its own genetically inferred ancestry grouping. It can be found in the file ending in "popsimilarity.txt.gz"
#Load in this file, and select for IID and Ancestry(which I rename from "MostSimilarPop")
Anc_info <- vroom::vroom("/project/damrauer_shared/Users/sabramow/PRS-Variability/PGS/allscores/PMBB/score/PMBB_popsimilarity.txt.gz") %>% 
  select(IID, Ancestry = MostSimilarPop)

# Load in the pgs.txt.gz file generated by pgsc_calc which contains individuals' PRSs for each scorefile. 
df_allscores <- vroom::vroom("/project/damrauer_shared/Users/sabramow/PRS-Variability/PGS/allscores/PMBB/score/PMBB_pgs.txt.gz") %>% 
  select(IID, PGS, Z_norm2) %>%  # select IID, PGS, and Z_norm2 files
  mutate(PGS = sub("_hmPOS_GRCh38$", "", PGS)) %>% # remove descriptive but excess text from from the deafault PGS names
  pivot_wider(.,names_from = PGS, values_from = Z_norm2) %>% # Pivot wider to make a column for each individual score
  merge(.,Anc_info, by = "IID") %>%  # Merge with ancestry data frame by individual ID
  rename(PGS_LDP2Auto = CAD_LDP2Auto_Weights) %>% # Specific to CAD PRS pipeline
  rename(PGS_prscsx = "CAD_PRS-CSx_META_Weights") # Specific to CAD PRS pipeline

# Load in file containing individual age and phenotype information
score_df <- vroom::vroom("/project/damrauer_shared/Users/sabramow/PRS-Variability/PMBB_CAD_Phenotypes.txt") %>%
  mutate(CAD_FLG = base::as.factor(CAD_FLG)) %>% # set phenotype flag to be a factor
  merge(.,df_allscores, by.x = "PMBB_ID", by.y = "IID" ) %>%  # merge this data frame with the previously made one with scores
  rename(PHENO_FLG = CAD_FLG) %>% # rename phenotype column to PHENO_FLG
  rename(Age = Age_first_consent) %>% 
  rename(Sex = Gen_Sex) %>% 
  rename(IID = PMBB_ID)

# Write table to output path for easy access later
write.table(score_df, file = paste0(path, database, pheno,  "_PGS.txt"), sep = "\t",
            row.names = FALSE, col.names = TRUE, quote = FALSE)
```

```{r}
# Directly read in properly formatted data frame
score_df <- vroom::vroom(paste0(path, database, pheno, "_PGS.txt")) %>% # path to previously generated file
  mutate(PHENO_FLG = base::as.factor(PHENO_FLG)) # make sure PHENO_FLG is still a factor
```

### Demographic Table

Table with breakdown of Ancestry and Sex stratified by outcome

```{r}
demo_table<- 
  score_df  %>% 
  tbl_summary(
    include= c(Ancestry, Sex),
    by = PHENO_FLG,
    label = PHENO_FLG ~ pheno) %>% 
  modify_header(label = pheno,
                stat_1 =  '**Unaffected**',
                stat_2 = '**Affected**') %>% 
  modify_caption("Summary")

demo_table


gt_table <- as_gt(demo_table)
gt::gtsave(gt_table, file = paste0(path,database,"demo_table.html"))
# To save picture of output, use webshot
```

# Analysis Pipeline:

The following code should run with minimal modification. Steps where modification is required or customization options exist will be noted.

::: {.callout-note appearance="simple"}
The analysis pipeline can be broken down into four steps

1.  Incorporate PRS into a model of binary outcome, evaluate statistical significance and Odds Ratio
2.  For scores that significantly positively associate with disease in the model, calculate model performance metrics: Brier Score and Area under Receiver Operator Characteristic Curve
3.  Identify score with 'practically equivalent' population-level model performance as that of the best-performing score, according to specified "equivalence" criteria
4.  Assess Variability of Individual Scores Provided by Equivalent Models
:::

## 1. Modeling

### 1a) glm

Fit generalized linear model (glm) to calculate odds ratio of phenotype per SD change in PRS and covariates

```{r}
# fit logistic regression models for each PRS to estimate OR per SD
glm_PGS <- function(score_df, formula_terms = c("PGS", "Age", "Sex")){ #takes formatted data frame (describe above) and list of variables to include in the model (default is inclusion of PGS, Age, and Sex)
  
  formula_str <- paste("PHENO_FLG ~", paste(formula_terms, collapse = " + "))
  
  prs_glm <- score_df %>% 
    pivot_longer(cols = starts_with("PGS"), values_to = "PGS", names_to = "version") %>%
    group_nest(version) %>% 
    dplyr::mutate(glm = map(data, ~ glm(as.formula(formula_str), family = "binomial", data = .))) %>% 
    dplyr::mutate(tidy = map(glm, ~ broom::tidy(., exponentiate = TRUE, conf.int = TRUE))) %>% 
    dplyr::mutate(glance = map(glm, ~ broom::glance(.))) %>%
    dplyr::select(version, glm, tidy, glance) %>%
    tidyr::unnest(c(tidy, glance)) %>% 
    select(version, term, estimate, std.error, p.value, conf.low, conf.high)
  
  return(prs_glm)
  
}
```

Implement glm function

```{r}
# Implement the above function. Here, using default df and model terms
prs_glm <- glm_PGS(score_df, formula_terms = c("PGS", "Age", "Sex"))

# Create a list of all PGSs, excluding those with PRSs that have negative OR or p>0.05 in model. Used to subset later.
pgs_columns <- prs_glm %>% 
  filter(.,term == "PGS") %>% 
  filter(., p.value <= 0.05) %>% 
  filter(., estimate >= 1) %>% 
  filter(grepl("^PGS", version)) %>%
  pull(version) %>%
  unique()
```

Make results of glm_PGS function into a table containing estimates, their confidence intervals, and P values

```{r}
glm_table <- prs_glm %>%
  mutate(across(where(is.numeric), ~ signif(., digits = 4))) %>%  # Four significant figures
  filter(., term != "(Intercept)") %>% 
  mutate(CI = paste0("(", conf.low, ", ", conf.high, ")")) %>% 
  select(version, term, estimate, CI, p.value) %>% 
  pivot_wider(., names_from = term, values_from = c(estimate, CI, p.value)) 


# Save table output
write.table(glm_table, file = paste0(path, database, "glmtable.txt"))
```

### 1b) Make Forest Plot with PRS OR

Make glm_forestplot function Description: take glm output from prs_glm and create a forest plot of Odds Ratios for a model term for each model

Inputs

-   prs_glm - data frame output of prs_glm
-   model_term (character) - term included in glm. For example "Age" "Sex" or "PGS"
-   sig_filter (logical) - TRUE or FALSE, indicate whether or not to filter out scores where the p value in the glm was \>0.05 and/or the term estimate is negative. Default is FALSE.
-   save_fig (logical) - TRUE or FALSE, indicate whether to save figure output with ggsave to specified output path. Default is TRUE.

```{r fig.align="center", echo = FALSE,fig.width = 6,fig.height =7.5}
glm_forestplot <- function(prs_glm, model_term, sig_filter = FALSE, save_fig = TRUE) {
  
  all_forest_prs <- prs_glm %>% 
    filter(term == as_string(model_term)) %>% # filter to model term of choice
    { if(sig_filter == TRUE) filter(., version %in% pgs_columns) else . } %>% # apply filter to include those models in pgs_columns if sig_filter is true
    ggplot(., aes(x = estimate, y = version, xmin = conf.low, xmax = conf.high)) +
      geom_point(position = position_dodge(width = 0.5), size = 3) +
      geom_errorbarh(height = 0.2, position = position_dodge(width = 0.5)) +
      geom_vline(xintercept = 1, linetype = "dashed", color = "#4C4E52", linewidth = 0.75) +  # Reference line at OR = 1
      labs(x = "Odds Ratio (OR)", y = "Score in Model") +
      theme_minimal() +
      theme(legend.position = "top") +
      theme(text = element_text(size = 10)) 
  
  print(all_forest_prs) # Plot ggplot
  
  if (save_fig == TRUE) {
    ggsave(paste(path, database, "glm_forestplot.png"))
   #save ggplot if save_fig is TRUE
}
  
}

glm_forestplot(prs_glm, "PGS", sig_filter = TRUE, save_fig = FALSE)

```

### 1c) Calculate Percentiles

Use prior outputs to make new data frame that has each individual's percentile according to each score. Based on normal distribution (Znorm2 has mean 0, SD 1)

```{r}

df_ntile_norm <- score_df %>% 
  pivot_longer(cols = starts_with("PGS"), values_to = "PGS", names_to = "version") %>%
  filter(version %in% pgs_columns) %>% 
  group_by(version) %>%
  dplyr::mutate(ntile = 100 * pnorm(PGS)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = version, values_from = c(PGS, ntile))

```

## 2. Model Performance Metrics

The code in this section can be quite time/resource-intensive, depending on number of individuals and number of scores. Consider testing this code on subsets of your data with fewer individuals/scores, and running the full version in a non-interactive manner.

### 2a) Model discrimination and calibration via Vfold Resample

Define list of scores to include in analysis

```{r}
pgs_include <- c(pgs_columns)  #Default - use previously generated pgs_columns to restrict to scores with + OR and p<0.05.'
# Alternatively, can define this list manually to include a curated subset of scores
```

Run vfold cross validation

```{r}
scores_vfold <- score_df

#parsnip
glm_model <- 
  logistic_reg() %>% 
  set_engine("glm")

#make workflow w/ parsnip model object
set.seed(2023) # Can replace with chosen seed

folds_x <- vfold_cv(scores_vfold, v = 10, repeats = 6, strata = NULL, pool = 0.1) # set number of folds/repeats
cls_met <- metric_set(roc_auc, brier_class) # define metrics of interest as brier score, AU ROC
control <- control_resamples(save_pred = TRUE, save_workflow = TRUE)

# Create empty data frame to store model info
models_df <- data.frame()
# Create an empty list to store the results for each predictor
roc_plot_list <- list()
cal_plot_list <- list()


# Loop through the predictors and create a workflow and fit for each
for (predictor in pgs_include) {
  
  set.seed(2023)
  
  name <- sub("prs_scaled_","",predictor)
  
  glm_workflow <- 
    workflow() %>% 
    add_model(glm_model) %>% 
    add_variables(outcome = PHENO_FLG, predictors = c(predictor, Age, Sex)) # Remove Age and Sex if looking to test null model with just outcome ~ PGS
  

  glm_fit_all <- 
    fit_resamples(glm_workflow, folds_x, metrics = cls_met, control = control) 
  
  model_metrics <- as_workflow_set (!!name := glm_fit_all) 
  
  if (nrow(models_df) == 0 ){
    models_df <- model_metrics
    
  } else{
    models_df <- bind_rows(model_metrics, models_df)

  }
  

  # Make ROC curve
  
  plot_title <- paste(predictor,"ROC", database)
  
  roc_data <- glm_fit_all %>% 
    collect_predictions() %>% 
    roc_curve(PHENO_FLG, .pred_1, event_level = "second") %>% 
    dplyr::mutate(predictor = predictor) %>% 
    autoplot() + ggtitle(plot_title)
  
  roc_plot_list[[predictor]] <- roc_data
  
  print(roc_data) # optional, will print all ROC curves
  

   #Make Calibration Plot
  
  cal_plot_title <- paste(predictor, "Calibration", database)
  
  cal_data <- glm_fit_all %>% 
    collect_predictions() %>% 
    calib_curve(truth = PHENO_FLG, estimate = .pred_1, event_level = "second") %>% 
    autoplot() + ggtitle(cal_plot_title) +  xlab("Predicted Probability") + ylab("Observed Probability")
  
  cal_plot_list[[predictor]] <- cal_data
  
  cal_plot_list[[predictor]] <- cal_data
  print(cal_data) # optional, will print all calibration plots

}
```

(optional) save figures

```{r}
# Save as condensed versions to optimize storage
plot_list_ROC <- lapply(roc_plot_list, function(l) {
  drop_vars(l)
}) %>% 
  saveRDS(., file = paste0(path,database,"roc_plot_list.rds"))


plot_list_cal <- lapply(cal_plot_list, function(l) {
  drop_vars(l)
}) %>% 
  saveRDS(., file = paste0(path,database,"cal_plot_list.rds"))

```

### 2b) Resampling for posterior distributions of performance metrics

::: {.callout-warning appearance="simple"}
8000 iterations was necessary in PMBB and AllofUs to avoid warnings regarding low bulk/tail effective sample size. The default for perf_mod is 2000. This takes a while (in my experience, minimum 30 minutes with 40 cores, plenty of memory) to run. Consider moving this to an R script and submitting it as a job (and saving perf_model_brier and perf_model_auc as .RDS objects) so you can walk away while this is running.
:::

```{r}
perf_model_brier <- perf_mod(models_df, metric = "brier_class", seed = 2023, refresh = 0, iter = 8000, cores =40)
perf_model_auc <- perf_mod(models_df, metric = "roc_auc", seed = 2023, refresh = 0, iter = 8000, cores = 40)
```

## 3. Model Comparison

### 3a) df with Comparative Model Performance

https://www.tmwr.org/compare.html

Identify score with 'practically equivalent' population-level model performance as that of the best-performing score, according to specified "equivalence" criteria. Currently looks at five definitions of equivalence:

-   prob_dif: \<95% Probability of positive difference between score X and best score (brier_A - best_brier, best_auc - auc_A)
-   CI_95: does the 95% credible interval of the posterior distribution of score A's metric overlap with the 95% credible interval of the posterior distribution of the 'best' metric?
-   ROPE_02: Is there a \>95% probability of practical equivalence with a ROPE of 0.02
-   ROPE_01: Is there a \>95% probability of practical equivalence with a ROPE of 0.01
-   ROPE_005: Is there a \>95% probability of practical equivalence with a ROPE of 0.005

```{r fig.align="center", echo = FALSE,fig.width = 9,fig.height =7.5}
# Brier
best_brier <- 
  tidy(perf_model_brier, seed = 2023) %>%
  summary() %>% 
  slice_min(mean) %>%
  pull(model)


contrast_models_brier_02 <- list()
contrast_models_brier_01 <- list()
contrast_models_brier_005 <- list()
post_diff_brier <- list()
post_diff_auc <- list()


for (i in pgs_include){
  
  diff <-  contrast_models(
    perf_model_brier,
    list_1 = i,
    list_2 = best_brier,
    seed = 2023) 
  
  model <- i
  
  brier_diff_fig <- diff %>% 
    as_tibble() %>% 
    ggplot(aes(x = difference)) + 
      labs(
      title = paste("Posterior Distribution of", model, "vs", best_brier, " Brier Scores in", database)) + 
      custom_theme +
    geom_vline(xintercept = 0, lty = 2) + 
    geom_histogram(bins = 50, color = "white", fill = "red", alpha = 0.4)
  
  post_diff_brier[[i]] <- brier_diff_fig
  
  contrast_models_brier_02[[i]] <- diff %>% 
    summary(., size = 0.02) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_02 = pract_equiv) %>% 
    select(model, pract_equiv_02, probability)
  
  contrast_models_brier_01[[i]] <- diff %>% 
    summary(., size = 0.01) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_01 = pract_equiv) %>% 
    select(model, pract_equiv_01)
  
  contrast_models_brier_005[[i]] <- diff %>% 
    summary(., size = 0.005) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_005 = pract_equiv) %>% 
    select(model, pract_equiv_005)
  
}

brier_ci <- perf_model_brier %>%
  broom::tidy() %>%
  summary(.,prob=0.95) %>% # summary, probability 95%
  mutate(CI_95 = ifelse(lower <= min(upper), 1, 0)) %>%  
  merge(.,(bind_rows(contrast_models_brier_005)), by = "model") %>% 
  mutate(ROPE_005 = ifelse(pract_equiv_005 < 0.95, 0, 1)) %>% 
  merge(.,(bind_rows(contrast_models_brier_01)), by = "model") %>%
  mutate(ROPE_01 = ifelse(pract_equiv_01 < 0.95, 0, 1)) %>% 
  merge(.,(bind_rows(contrast_models_brier_02)), by = "model") %>% 
  mutate(ROPE_02 = ifelse(pract_equiv_02 < 0.95, 0, 1)) %>% 
  mutate(prob_dif = ifelse(probability>=0.95,0,1)) %>% 
  mutate(metric = "Brier Score") %>% 
  mutate(.,
         ROPE_color = as.character(ROPE_005 + ROPE_01 + ROPE_02)
         ) %>% 
  arrange(desc(mean)) 


 # AUC
best_auc <- 
  tidy(perf_model_auc, seed = 2023) %>%
  summary() %>% 
  slice_max(mean) %>%
  pull(model)

contrast_models_auc_02 <- list()
contrast_models_auc_01 <- list()
contrast_models_auc_005 <- list()

for (i in pgs_include){
  
  diff <-  contrast_models(
    perf_model_auc,
    list_1 = best_auc,
    list_2 = i,
    seed = 2023) 
  
  model <- i
  
  auc_diff_fig <- diff %>% 
    as_tibble() %>% 
    ggplot(aes(x = difference)) + 
      labs(
      title = paste("Posterior Distribution of", best_auc, "vs", model, "AU ROC in", database)) + 
      custom_theme +
    geom_vline(xintercept = 0, lty = 2) + 
    geom_histogram(bins = 50, color = "white", fill = "red", alpha = 0.4)
  
  
  post_diff_auc[[i]] <- auc_diff_fig
  
  contrast_models_auc_02[[i]] <- diff %>% 
    summary(., size = 0.02) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_02 = pract_equiv) %>% 
    select(model, pract_equiv_02, probability)
  
  contrast_models_auc_01[[i]] <- diff %>% 
    summary(., size = 0.01) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_01 = pract_equiv) %>% 
    select(model, pract_equiv_01)
  
  contrast_models_auc_005[[i]] <- diff %>% 
    summary(., size = 0.005) %>% 
    mutate(model = i) %>% 
    rename(pract_equiv_005 = pract_equiv) %>% 
    select(model, pract_equiv_005)
}


# Probability = proportion of the posterior that is > 0 (probability that the positive difference is real)

auc_ci <- perf_model_auc %>%
  broom::tidy() %>%
  summary(.,prob=0.95) %>% # summary, probability 95%
  mutate(CI_95 = ifelse(upper >= max(lower), 1, 0)) %>%  
  merge(.,(bind_rows(contrast_models_auc_005)), by = "model") %>% 
  mutate(ROPE_005 = ifelse(pract_equiv_005 < 0.95, 0, 1)) %>% 
  merge(.,(bind_rows(contrast_models_auc_01)), by = "model") %>%
  mutate(ROPE_01 = ifelse(pract_equiv_01 < 0.95, 0, 1)) %>% 
  merge(.,(bind_rows(contrast_models_auc_02)), by = "model") %>% 
  mutate(ROPE_02 = ifelse(pract_equiv_02 < 0.95, 0, 1)) %>% 
  mutate(prob_dif = ifelse(probability>=0.95,0,1)) %>% 
  mutate(metric = "AU ROC") %>% 
  mutate(.,
         ROPE_color = as.character(ROPE_005 + ROPE_01 + ROPE_02)
         ) %>% 
  arrange(desc(mean)) 

  

# AUC AND Brier
model_metrics_df <- rbind(brier_ci, auc_ci)
# Save output
write.table(model_metrics_df, file = paste0(path, database, "model_metrics_df.txt"))
```

Plot, save plots of posterior distributions

```{r}
print(post_diff_auc)
saveRDS(post_diff_auc, file = paste0(path,database,"post_diff_auc_plots.rds"))
print(post_diff_brier)
saveRDS(post_diff_brier, file = paste0(path,database,"post_diff_brier_plots.rds"))
```

### 3b) Forest Plotting

```{r}
model_metrics_plotCI <- model_metrics_df %>%
  mutate(sig_color = as.character(prob_dif + ROPE_02)) %>% 
  group_by(metric) %>% 
  mutate(name=factor(model, levels=model)) %>%
  ungroup() %>% 
  ggplot(., aes(x = mean, y = name, xmin = lower, xmax = upper, color = sig_color)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbarh(height = 0.2, position = position_dodge(width = 0.5)) +
  labs(x = "95% Credible Interval ", y = "Score") +
  theme_light() +
  theme(legend.position = "top") +
  scale_color_manual(values = c("0" = "black", "1" = "red", "2" = "blue"), name = "", 
                     labels = c("0" = "Significantly Different", "1" = "Practically Equivalent (ROPE 0.02)", "2" = "Probability of Real Difference"))  +
  facet_wrap(~ metric, scales = "free_x") +
  theme(strip.text = element_text(size = 14, colour = 'black'))


model_metrics_plotCI
ggsave(paste(path, database, "_model_metrics_plotCI.png"))
```

### 3c) Get list of "Equivalent" Scores

Re-order scores. Below code is specific to case of evaluating CAD PRSs including PGS_LDP2Auto and PGS_prscsx

```{r}
# Sort in order of new-ness (use with CAD pipeline)
custom_sort <- function(x) {
  ordered <- x[grepl("^PGS0", x)]  # Select elements starting with "PGS0"
  special <- x[grepl("PGS_LDP2Auto|PGS_prscsx", x)]  # Select special elements
  ordered <- ordered[order(as.numeric(gsub("PGS", "", ordered)))]  # Order based on numeric part
  c(ordered, special)  # Combine ordered and special elements
}
```

Function to create list of equivalent scores according to different criteria Inputs

-   df data frame, result of 3a
-   criteria - (string) one of five equivalence criteria specified in description of 3a (prob_dif, CI_95, ROPE_02, ROPE_01, ROPE_005)
-   ntile (logical) - TRUE or FALSE, indicate whether or not the list should append ntile to name. Used in a few downstream functions. Default is FALSE.

```{r}
equiv_scores <- function(df, criteria, ntile = FALSE) {
  # Ensure criteria is a column in df
  if (!criteria %in% names(df)) {
    #stop("Criteria column not found in dataframe")
    stop(paste0("Criteria column ", criteria, " not found in dataframe"))
  }
  
  # Filter models based on criteria for each group
  models <- df %>%
    group_by(model) %>%
    filter(all(.data[[criteria]] == 1)) %>%
    ungroup() %>%
    pull(model) %>%
    unique()
  
  # Sort models
  sorted_models <- custom_sort(models)
  
  # Return the appropriate list based on ntile flag
  if (ntile) {
    ntile_list <- paste("ntile_", sorted_models, sep = "")
    return(ntile_list)
  } else {
    return(sorted_models)
  }
}

```

## 4. Individual Variability

### 4a) Plot for Individuals

Scores Select five individuals, plot their score percentile according to each score that fits the ROPE 0.02 equivalence criteria, in the order the scores have been released

plot_indiv_scores function Inputs:

-   df_ntile - data frame, df_ntile_norm
-   all_metrics_df - data frame, output of 3a (model_metrics_df)
-   criteria - (string) one of five equivalence criteria specified in description of 3a (prob_dif, CI_95, ROPE_02, ROPE_01, ROPE_005). Default is "ROPE_02"
-   n_indiv - number of individuals to plot. Default is 5
-   seed - seed used to randomly select individuals to plot. Default is 2023

```{r fig.align="center", echo = FALSE,fig.width = 12.5,fig.height =4.5}

# Make into a function
plot_indiv_scores <- function(df_ntile, all_metrics_df, criteria = "ROPE_02", n_indiv = 5, seed = 2023){
  
  ntile_list <- equiv_scores(all_metrics_df, criteria , ntile = TRUE)
  model_list <- equiv_scores(all_metrics_df, criteria , ntile = FALSE)
  
  random_ntile <- sample_n(df_ntile, n_indiv) %>% 
    select(IID, ntile_list)
  
  melt_random_ntile <- melt(random_ntile, id = c("IID"))  %>% 
    mutate(variable = str_replace(variable, "ntile_", ""))
  
  melt_random_ntile$variable <- factor(melt_random_ntile$variable, levels = model_list)
  
  score_plot_ntile_all <- ggplot(data = melt_random_ntile, aes(x = variable, y = value, color = IID, group = IID)) +
    geom_line(size = 1.25) +
    labs(x = "Score", y = "Percentile") + 
    custom_theme +
    theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
    scale_color_jama(guide = "none") 
  
  return(score_plot_ntile_all)
}


indiv_plot <- plot_indiv_scores(df_ntile_norm, model_metrics_df)
indiv_plot
ggsave(paste(path, database, "indiv_plot.png"))

```

### 4b) Individual risk, averaged across equivalent scores

Calculate the mean, SD, and coefficient of variation of an individual's score percentile, across equivalent scores

```{r}

make_sumscores_pivot <- function(df_ntile_norm, all_metrics_df = model_metrics_df,  criteria = "ROPE_02"){
  
  ntile_list <- equiv_scores(all_metrics_df, criteria , ntile = TRUE)
  model_list <- equiv_scores(all_metrics_df, criteria , ntile = FALSE)
  pgs_list<- paste("PGS_", model_list, sep = "") 
  
  sumscores_pivot <- df_ntile_norm %>% 
    select(IID, Age, Sex, Ancestry, PHENO_FLG, ntile_list, pgs_list) %>% 
    mutate(!!pheno := base::as.factor(PHENO_FLG)) %>%
    pivot_longer(cols = starts_with("PGS"), names_to = "PGS_method", values_to = "PGS") %>% 
    pivot_longer(cols = starts_with("ntile"), names_to = "ntile_method", values_to = "ntile")
  
  return(sumscores_pivot)
}

sumscores_pivot <- make_sumscores_pivot(df_ntile_norm)

sum_ntile <- sumscores_pivot %>% 
  desc_statby(., measure.var = "ntile", grps = c("IID"))
```

```{r fig.align="center", echo = FALSE,fig.width = 4,fig.height =5}

sum_metrics <- c("mean", "sd", "cv")
metric_labels <- c("mean" = "Mean", "sd" = "Standard Deviation", "cv" = "Coefficient of Variation")


# density plot
for (metric in sum_metrics) {
  metric_label <- metric_labels[metric]
  plot <- ggplot(sum_ntile, aes(x = .data[[metric]])) + geom_density(alpha = .7, color = "#B24745FF",  fill = "#B24745FF") + 
  labs(
    title = paste("PRS Percentile", metric_label),
    x = paste("PRS Percentile", metric_label),
    y = "Density"
  ) +
     scale_color_identity() + custom_theme + theme(legend.position = "none")
  print(plot)
}

```

Calculate medians and confidence intervals

```{r}
library(simpleboot)
library(boot)
sum_metrics <- c("mean", "sd", "cv")

avg_stats <- lapply(sum_metrics, function(met) {
  boot_med <- simpleboot::one.boot(sum_ntile[[met]], median, R=1000)
  ci_result <- boot::boot.ci(boot_med, conf=0.95, type="norm")

  # Adjust this part based on the structure of 'ci_result'
  # For example, if 'ci_result' has elements named 'normal', 'basic', etc.
  # Extract the desired confidence interval bounds from the correct sub-element
  lower_bound <- ci_result$normal[[2]]  
  upper_bound <- ci_result$normal[[3]]
  median <- ci_result$t0

  df_result <- data.frame(Metric = met, 
                          Lower = lower_bound, 
                          Upper = upper_bound,
                          median = median)
  return(df_result)
}) %>% 
  bind_rows(.)


write.table(avg_stats, file = paste0(path, database, "avgstats.txt"))
avg_stats
```

### 4c) Intraclass Correlation
Function to calculate intraclass correlation (ICC)

```{r}
library(irr)

ICC_equiv <- function(df_ntile_norm,  model_metrics_df, criteria) {
  list <- equiv_scores(model_metrics_df, criteria, ntile = TRUE)
  
  df_ntile_criteria <- df_ntile_norm %>% 
    select(list) %>% 
    icc(., model = "twoway", type = "agreement")
  
  df_ntile_criteria$metric <- criteria
  
  df_ntile_tibble <- tibble(
    icc = df_ntile_criteria$value,
    ubound = df_ntile_criteria$ubound,
    lbound = df_ntile_criteria$lbound,
    Raters = df_ntile_criteria$raters,
    Criteria = df_ntile_criteria$metric)
  
  return(df_ntile_tibble)
}

metrics <- c("CI_95","prob_dif", "ROPE_005", "ROPE_01", "ROPE_02")

ICC_res <- lapply(metrics, function(m){
  ICC_equiv(df_ntile_norm = df_ntile_norm, model_metrics_df = model_metrics_df, criteria = m)
}) %>% 
  bind_rows(.)

write.table(ICC_res, file = paste0(path, database, "ICC.txt"))

```

### 4d) Light's Kappa

Note that in criteria here, criteria used must be met by at least two scores. 
Has relatively long run time. 

```{r}
create_binary_df <- function(df, list, percentile) {
  
  df_ntile_top <- df %>% 
    select(IID, list) %>% 
    pivot_longer(cols = starts_with("ntile"), names_to = "score", values_to = "ntile")
  
  for (i in seq_along(percentile)) {
    binary_col <- paste0("ntile_", percentile[i])
    
    df_ntile_top <- df_ntile_top %>%
      mutate(!!binary_col := ifelse(ntile >= percentile[i], 1, 0))
  }
  
  return(df_ntile_top)
}

calculate_and_kappa <- function(df_ntile_top, percentile, ntile_list, metric_name) {
    binary_col <- paste0("ntile_", percentile)

    df_binary <- df_ntile_top %>%
        dplyr::select(score, !!binary_col, IID) %>%
        pivot_wider(names_from = score, values_from = !!binary_col) %>%
        dplyr::select(all_of(ntile_list))

    kappa_value <- as.numeric(kappam.light(df_binary)$value)

    return(data.frame(metric = metric_name, percentile = percentile, kappa_value = kappa_value))
}


process_list <- function(metric) {
    list_data <- equiv_scores(model_metrics_df, metric, ntile = TRUE)
    df_ntile_top <- create_binary_df(df_ntile_norm, list_data, percentiles)

    results <- lapply(percentiles, function(p) {
        calculate_and_kappa(df_ntile_top, p, list_data, metric)
    })

    return(do.call(rbind, results))
}


#metrics <- c("ROPE_005", "ROPE_01", "ROPE_02", "prob_dif")
metrics <- c("ROPE_005")
percentiles <- c(99, 95, 90, 80, 70, 50)
kappa_results <- lapply(metrics, process_list)
combined_kappa_results <- do.call(rbind, kappa_results)


```

```{r}

# View the final structured data frame
print(combined_kappa_results)
write.table(combined_kappa_results, file = paste0(path, database, "LightsKappa.txt"))
```

#Extra figures

## PRS Density Plot Stratified by Outcome

```{r fig.align="center", echo = FALSE,fig.width = 4,fig.height =3}

# Function to create plots for each PRS
create_PRSdensplot <- function(data, prs_column) {
  
  p <-  ggplot(data, aes_string(x = prs_column, color = "PHENO_FLG")) +
    geom_density() +
    labs(
      title = paste("Density Plot of", prs_column, "by", pheno),
      subtitle = "Z_norm2",
      x = paste(prs_column),
      y = "density"
    ) +
    scale_color_manual(values = c("0"="#011F5B", "1"="#990000"), name = pheno) +
    custom_theme
  
  p.filt <-  drop_vars(p)
  p.filt
}

# Creating plots for each PRS
plot_list <- map(pgs_columns, ~ create_PRSdensplot(score_df, .x))

# Viewing plots
plot_list
```

## Correlation Plot

```{r}
ntile_cor <- df_ntile_norm %>% 
  correlation::correlation(p_adjust = "none") %>% 
  mutate(Parameter1 = str_replace(Parameter1, "ntile_", "")) %>% 
  mutate(Parameter2 = str_replace(Parameter2, "ntile_", ""))

plot_corr2 <- ntile_cor %>% 
  as.matrix() %>% 
  corrr::as_cordf() %>% 
  corrr::rearrange(method = "HC") %>%
  corrr::stretch() %>%
    left_join(bind_rows(
      ntile_cor %>%
                as_tibble() %>%
                select(x = Parameter1, y = Parameter2, p),
              ntile_cor %>%
                as_tibble() %>%
                select(x = Parameter2, y = Parameter1, p)
              ) %>%
  unique()) %>% 
  rename(pval = p) %>% 
    mutate(
      x = stringr::str_replace(x, "_", " "),
      y = stringr::str_replace(y, "_", " ")
    ) %>%
    mutate(
      x = forcats::fct_inorder(x),
      y = forcats::fct_inorder(y)
    ) %>%
    mutate(r = dplyr::case_when(is.na(r) ~ 1, TRUE ~ r)) %>%
    mutate(pval_bonferroni = pval < 0.05 / ((dplyr::n_distinct(.$x)^2 - dplyr::n_distinct(.$x)) / 2)) %>%
    mutate(label = dplyr::case_when(
      pval_bonferroni ~ as.character(glue::glue("P < {signif(0.05 / ((dplyr::n_distinct(.$x)^2 - dplyr::n_distinct(.$x))/2), 2)}")),
      TRUE ~ NA_character_
    ))


  
ntile_plot <- plot_corr2 %>%
    ggplot(aes(x, y, fill = r)) +
    geom_tile() +
    #geom_point(aes(shape = label)) +
    scale_fill_gradient2(low = "#2c477a", mid = "white", high = "#ad171c", limits = c(-1, 1), name = "*r*") +
    scale_shape_manual(values = c(8), na.translate = F, name = NULL) +
    guides(fill = guide_colourbar(order = 1)) +
    labs(
      x = NULL,
      y = NULL,
      #title = glue::glue("Top {(1-top_ntile)*100}%")
      title = ("percentile correlation")
    ) +
    coord_equal() +
    theme_bw(base_size = 16) +
    theme(
      panel.grid.major = element_blank(),
      plot.background = element_rect(fill = "transparent", color = NA),
      axis.text.x = element_text(angle = 90, hjust = 1, vjust = 1),
      legend.title = ggtext::element_markdown()
    )

ntile_plot


#check range
check_range <- plot_corr2 %>% 
  filter (r != 1) %>% 
  arrange(-r)
```

## Above and Below Thresholds

```{r}

ntile_thresh <- function(df_ntile, criteria, all_metrics_df){
  
  ntile_list <- equiv_scores(all_metrics_df, criteria , ntile = TRUE)
  model_list <- equiv_scores(all_metrics_df, criteria , ntile = FALSE)
  
  ntile_df_thresh <- df_ntile %>% 
    select(IID, ntile_list)  %>% # ntile list is list of score names with ntile prefix to include
    pivot_longer(cols = -IID) %>% 
    mutate(name = str_replace(name, "ntile_", "")) %>% 
    filter(name %in% model_list) %>% 
    crossing(ntile = c(5, 10, 20)) %>% 
    group_by(IID, ntile) %>% 
    summarize(above = sum(value > 100-ntile), below = sum(value < ntile)) %>% 
    mutate(combined = above > 0 & below > 0) %>% 
    group_by(ntile) %>% 
    summarize(prop_simultaneous = sum(combined)/n()) %>% 
    mutate(criteria = criteria)
  
  return(ntile_df_thresh)
}


metrics <- c("prob_dif", "CI_95", "ROPE_005", "ROPE_01", "ROPE_02")
above_below <- lapply(metrics, function(m){
  ntile_thresh(df_ntile = df_ntile_norm, criteria = m, all_metrics_df = model_metrics_df)
}) %>% 
  bind_rows(.)

write.table(above_below, file = paste0(path, database, "above_and_below.txt"))

```
